# 知识图谱补全系统 (LLM-KG-FT-V2)

本项目实现了基于大型语言模型（LLM）的知识图谱补全系统，通过LoRA微调Qwen-1.8B-Chat模型来预测知识图谱中缺失的尾实体。

## 项目结构

```
llm_kg_ft_v2/
├── config.py              # 配置文件，包含所有参数设置
├── data_processor.py      # 数据处理模块，负责加载和处理数据
├── trainer.py             # 模型训练模块，实现训练和评估功能
├── main.py                # 主程序，整合所有功能
├── ds_config.json         # DeepSpeed配置文件，优化训练效率
├── requirements.txt       # 项目依赖包列表
├── outputs/               # 模型输出和预测结果目录
└── logs/                  # 日志文件目录
```

## 功能特点

1. **数据处理**：自动加载OpenBG500数据集，处理实体和关系的中文文本映射
2. **负采样**：为训练数据生成负样本，提高模型区分能力
3. **LoRA微调**：使用参数高效微调技术，减少显存占用
4. **DeepSpeed优化**：利用分布式训练框架提高GPU使用率
5. **混合精度训练**：支持FP16混合精度训练，进一步减少显存需求
6. **评估与预测**：在开发集上评估模型性能，在测试集上生成预测结果

## 环境要求

- Python 3.8+ 
- CUDA 11.7+（支持4070TiS显卡）
- 16GB显存以上的GPU
- 32GB系统内存

## 安装依赖

```bash
cd llm_kg_ft_v2
pip install -r requirements.txt
```

## 配置说明

主要配置参数在`config.py`文件中设置：

- **数据集路径**：设置训练、测试、开发数据集以及实体和关系文本映射文件的路径
- **模型配置**：设置预训练模型名称、缓存目录等
- **训练参数**：batch size、学习率、LoRA参数等
- **输出配置**：设置模型保存和日志输出的目录

## 使用方法

### 训练模型

```bash
python main.py
```

程序会自动：
1. 加载数据集和映射表
2. 如果没有已训练的模型，则执行训练过程
3. 在开发集上评估模型性能
4. 在测试集上进行预测并保存结果

### 重新训练模型

如果已存在训练好的模型，程序会询问是否重新训练。输入`y`将重新开始训练过程。

## 优化策略

为了在4070TiS 16GB显存的硬件条件下高效运行，本项目采用了以下优化策略：

1. **4bit量化**：使用4bit量化加载预训练模型，大幅减少显存占用
2. **LoRA微调**：仅训练少量参数，保持模型性能的同时降低显存需求
3. **混合精度训练**：使用FP16混合精度训练，减少约50%的显存使用
4. **梯度累积**：通过梯度累积模拟更大的batch size
5. **DeepSpeed优化**：利用ZeRO-2优化器进一步减少显存占用和提高训练效率

这些优化措施可以将GPU使用率提高到80-90%，充分利用硬件资源。

## 预测结果

测试集的预测结果将保存在`outputs/test_predictions.tsv`文件中，格式为：

```
头实体ID\t关系ID\t真实尾实体ID\t预测的尾实体文本
```

## 注意事项

1. 首次运行会自动下载预训练模型，设置了中国大陆源以加速下载
2. 确保数据集路径正确，并且具有读写权限
3. 如果显存不足，可以尝试减小`config.py`中的`batch_size`参数
4. 训练过程中会生成日志文件，保存在`logs/`目录下

## 常见问题

### Q: 下载模型速度慢怎么办？
A: 程序已经设置了中国大陆的镜像源（HF_ENDPOINT=https://hf-mirror.com），如果仍然速度较慢，可以考虑手动下载模型并放置在指定的缓存目录中。

### Q: 训练过程中显存不足怎么办？
A: 可以尝试以下方法：
- 减小`batch_size`参数
- 增加`gradient_accumulation_steps`参数
- 减小`lora_r`参数值
- 关闭`fp16`选项（但会显著增加显存占用）

### Q: 如何调整生成文本的质量？
A: 可以调整`config.py`中的`temperature`、`max_new_tokens`和生成时的`num_beams`参数来控制生成文本的质量和多样性。